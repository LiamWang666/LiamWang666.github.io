<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liamwang666.github.io/"/>
  <updated>2019-12-16T05:38:39.789Z</updated>
  <id>https://liamwang666.github.io/</id>
  
  <author>
    <name>Liam Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Libsvm交叉验证与网格搜索（参数选择）</title>
    <link href="https://liamwang666.github.io/2019/12/14/Libsvm%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E4%B8%8E%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%EF%BC%88%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%EF%BC%89/"/>
    <id>https://liamwang666.github.io/2019/12/14/Libsvm%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E4%B8%8E%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%EF%BC%88%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%EF%BC%89/</id>
    <published>2019-12-14T11:02:01.000Z</published>
    <updated>2019-12-16T05:38:39.789Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><blockquote><p>转载自<a href="https://www.cnblogs.com/ranjiewen/p/6214425.html" target="_blank" rel="noopener" title="libsvm交叉验证与网格搜索（参数选择）">Libsvm交叉验证与网格搜索（参数选择）</a></p></blockquote><h1 id="一、交叉验证"><a href="#一、交叉验证" class="headerlink" title="一、交叉验证"></a>一、交叉验证</h1><p>交叉验证（Cross validation）是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）的方法， 能够避免过拟合问题。</p><p>交叉验证一般要尽量满足：</p><ul><li><p>训练集的比例要足够多，一般大于一半</p></li><li><p>训练集和测试集要均匀抽样</p></li></ul><a id="more"></a>  <p>交叉验证主要分成以下几类：</p><h4 id="1）Double-cross-validation"><a href="#1）Double-cross-validation" class="headerlink" title="1）Double cross-validation"></a>1）Double cross-validation</h4><p>Double cross-validation也称2-fold cross-validation(2-CV)，作法是<strong>将数据集分成两个相等大小的子集，进行两回合的分类器训练</strong>。在第一回合中，一个子集作为训练集，另一个作为测试集；在第二回合中，则将训练集与测试集对换后，再次训练分类器，而其中我们比较关心的是两次测试集的识别率。不过在实际中2-CV并不常用，主要原因是训练集样本数太少，通常不足以代表母体样本的分布，导致测试阶段识别率容易出现明显落差。此外，2-CV中子集的变异度大，往往无法达到「实验过程必须可以被复制」的要求。</p><h4 id="2）k-folder-cross-validation-k折交叉验证"><a href="#2）k-folder-cross-validation-k折交叉验证" class="headerlink" title="2）k-folder cross-validation(k折交叉验证)"></a>2）k-folder cross-validation(k折交叉验证)</h4><p>K-fold cross-validation (k-CV)则是Double cross-validation的延伸，做法是<strong>将数据集分成k个子集，每个子集均做一次测试集，其余的作为训练集。k-CV交叉验证重复k次，每次选择一个子集作为测试集，并将k次的平均交叉验证识别率作为结果</strong>。<br>优点：所有的样本都被作为了训练集和测试集，每个样本都被验证一次。10-folder通常被使用。</p><h4 id="3）leave-one-out-cross-validation-LOOCV留一验证法"><a href="#3）leave-one-out-cross-validation-LOOCV留一验证法" class="headerlink" title="3）leave-one-out cross-validation(LOOCV留一验证法)"></a>3）leave-one-out cross-validation(LOOCV留一验证法)</h4><p>假设数据集中有n个样本，那LOOCV也就是n-CV，意思是每个样本单独作为一次测试集，剩余n-1个样本则做为训练集。<br>优点：</p><ul><li>每一回合中几乎所有的样本皆用于训练model，因此最接近母体样本的分布，估测所得的generalization error比较可靠。 因此在实验数据集样本较少时，可以考虑使用LOOCV。</li><li>实验过程中没有随机因素会影响实验数据，确保实验过程是可以被复制的。</li></ul><p>但LOOCV的缺点则是计算成本高，为需要建立的models数量与总样本数量相同，当总样本数量相当多时，LOOCV在实作上便有困难，除非每次训练model的速度很快，或是可以用平行化计算减少计算所需的时间。</p><blockquote><p>libsvm提供了 void svm_cross_validation(const struct svm_problem *prob, const struct svm_parameter *param, int nr_fold, double *target)方法，参数含义如下：</p></blockquote><pre><code>- prob：待解决的分类问题，就是样本数据。- param：svm训练参数。- nr_fold：顾名思义就是k折交叉验证中的k，如果k=n的话就是留一法了。- target：预测值，如果是分类问题的话就是类别标签了。</code></pre><hr><h1 id="二、参数选择"><a href="#二、参数选择" class="headerlink" title="二、参数选择"></a>二、参数选择</h1><p>使用svm，无论是libsvm还是svmlight，都需要对参数进行设置。以RBF核为例，在《A Practical Guide to Support Vector Classi cation》一文中作者提到在RBF核中有2个参数：C和g。对于一个给定的问题，我们事先不知道C和g取多少最优，因此我们要进行模型选择（参数搜索）。这样做的目标是找到好的(C, g)参数对，使得分类器能够精确地预测未知的数据，比如测试集。需要注意的是在训练集上追求高精确度可能是没用的（意指泛化能力）。根据前一部分所说的，<strong>衡量泛化能力要用到交叉验证</strong>。</p><p>在文章中作者推荐使用“网格搜索”来寻找最优的C和g。所谓的<strong>网格搜索就是尝试各种可能的(C, g)对值，然后进行交叉验证</strong>，找出使交叉验证精确度最高的(C, g)对。“网格搜索”的方法很直观但是看起来有些原始。事实上有许多高级的算法，比如可以使用一些近似算法或启发式的搜索来降低复杂度。但是我们倾向于使用“网格搜索”这一简单的方法:</p><ul><li>1）从心理上讲，不进行全面的参数搜索而是使用近似算法或启发式算法让人感觉不安全。</li><li>2）如果参数比较少，“网格搜索”的复杂度比高级算法高不了多少。</li><li>3）“网格搜索”可并行性高，因为每个(C, g)对是相互独立的。</li></ul><p>说了那么大半天，其实<strong>“网格搜索”就是n层循环</strong>，n是参数个数，仍然以RBF核为例，编程实现如下：</p><pre><code>for(double c=c_begin;c&lt;c_end;c+=c_step){         for(double g=g_begin;g&lt;g_end;g+=g_step)        {         //这里进行交叉验证，计算精确度。        }}</code></pre><p>通过上述两层循环找到最优的C和g就可以了。</p><h1 id="三、注意点"><a href="#三、注意点" class="headerlink" title="三、注意点"></a>三、注意点</h1><ul><li><p>使用Cross-Validation时常犯的错误</p><p>  由于实验室许多研究都有用到evolutionary algorithms(EA)与classifiers，所使用的fitness function中通常都有用到classifier的辨识率，然而把cross-validation用错的案例还不少。前面说过，只有training data才可以用于model的建构，所以<strong>只有training data的辨识率才可以用在fitness function中</strong>。而EA是训练过程用来调整model最佳参数的方法，所以只有在EA结束演化后，model参数已经固定了，这时候才可以使用test data。（当然如果想造假的话就把测试集的数据参与进模型训练，这样得到的模型效果多少会好些，因为模型本身已经包含了测试集的先验知识，测试集对它来说不再是未知数据。）</p></li><li><p>那EA跟cross-validation要如何搭配呢？</p><p>  Cross-validation的本质是用来估测(estimate)某个classification method对一组dataset的generalization error，不是用来设计classifier的方法，所以<strong>cross-validation不能用在EA的fitness function中</strong>，因为与fitness function有关的样本都属于training set，那试问哪些样本才是test set呢？如果某个fitness function中用了cross-validation的training或test辨识率，那么这样的实验方法已经不能称为 cross-validation了。</p></li><li><p>EA与k-CV正确的搭配方法</p><p>  将dataset分成k等份的subsets后，每次取1份 subset作为test set，其余k-1份作为training set，并且将该组training set套用到EA的fitness function计算中(至于该training set如何进一步利用则没有限制)。因此，正确的k-CV 会进行共k次的EA演化，建立k个classifiers。而k-CV的test辨识率，则是k组test sets对应到EA训练所得的k个classifiers辨识率之平均值。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;转载自&lt;a href=&quot;https://www.cnblogs.com/ranjiewen/p/6214425.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;libsvm交叉验证与网格搜索（参数选择）&quot;&gt;Libsvm交叉验证与网格搜索（参数选择）&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;一、交叉验证&quot;&gt;&lt;a href=&quot;#一、交叉验证&quot; class=&quot;headerlink&quot; title=&quot;一、交叉验证&quot;&gt;&lt;/a&gt;一、交叉验证&lt;/h1&gt;&lt;p&gt;交叉验证（Cross validation）是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）的方法， 能够避免过拟合问题。&lt;/p&gt;
&lt;p&gt;交叉验证一般要尽量满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;训练集的比例要足够多，一般大于一半&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;训练集和测试集要均匀抽样&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="支撑向量机SVM" scheme="https://liamwang666.github.io/tags/%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E6%9C%BASVM/"/>
    
      <category term="机器学习" scheme="https://liamwang666.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支撑向量机学习笔记</title>
    <link href="https://liamwang666.github.io/2019/12/14/%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://liamwang666.github.io/2019/12/14/%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2019-12-14T05:18:16.000Z</published>
    <updated>2019-12-17T04:49:57.134Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>🎵听说音乐和学习更配哟🎵</p><pre><code>【注：本文侧重应用，不讲推导】</code></pre><hr><h1 id="1-SVM概述"><a href="#1-SVM概述" class="headerlink" title="1. SVM概述"></a>1. SVM概述</h1><ul><li>类型：有监督学习</li><li>目的：在高维或无限维空间中构造一个超平面或一组超平面，可用于分类、回归或其他任务。</li></ul><blockquote><p>若样本为 n 维，则超平面为 n-1 维</p></blockquote><a id="more"></a>  <ul><li><p>样本可通过超平面实现良好的分离，该平面与所有类别的最近训练数据点的距离最大<strong>（最小距离最大化）</strong>。通常最大间隔 margin 越大，则SVM的泛化误差越小。</p><div align=center><img src="https://i.loli.net/2019/12/14/g2rQS5pvOdWeitM.png" width = "30%" height = "25%" alt="SMOTE.png"></li><li><div align=left>任意超平面均可表示为 wx+b=0，其中 w 为平面的法向量，我们需找到满足要求的 w 和 b，使得最大间隔 margin 最大化。w 越小，margin 越大，等价条件如下：<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRnlHx.png" width = "25%" height = "7%" alt="SMOTE.png"></li></ul><hr><h1 id="2-SVM解决分类问题"><a href="#2-SVM解决分类问题" class="headerlink" title="2. SVM解决分类问题"></a>2. SVM解决分类问题</h1><blockquote><div align=left>对偶形式为原始不等式形式的定式表达</blockquote><h2 id="1-1-线性二分类问题"><a href="#1-1-线性二分类问题" class="headerlink" title="1.1 线性二分类问题"></a><div align=left>1.1 线性二分类问题</h2><ul><li><div align=left>原始形式<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRuIOA.png" width = "25%" height = "8%" alt="SMOTE.png"></li><li><div align=left>对偶形式<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRu7wt.png" width = "30%" height = "10%" alt="SMOTE.png"></li><li><div align=left>决策函数<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRu4QH.png" width = "30%" height = "10%" alt="SMOTE.png"></li></ul><div align=left>其中，yi为样本标签，b为阈值，αi为对应样本点的拉格朗日系数。对于每一个样本，都需要带入上式求 f(x)<div align=left><h2 id="1-2-非严格线性二分类问题"><a href="#1-2-非严格线性二分类问题" class="headerlink" title="1.2 非严格线性二分类问题"></a><div align=left>1.2 非严格线性二分类问题</h2><ul><li><div align=left>原始形式<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRuqFf.png" width = "25%" height = "11%" alt="SMOTE.png">  <div align=left>C为损失函数的乘法因子，用于衡量异常样本的容忍度（重视度）。min 整体一定，如果C很大则 min 前一部分很小，容错空间则更小，表示该样本很重要，不允许分类错误。</li><li><div align=left>对偶形式<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRuWWD.png" width = "30%" height = "12%" alt="SMOTE.png"></li></ul><h2 id="1-3-非线性问题（核函数）"><a href="#1-3-非线性问题（核函数）" class="headerlink" title="1.3 非线性问题（核函数）"></a><div align=left>1.3 非线性问题（核函数）</h2><div align=left>找不到一个线性可分面，则可运用核函数解决。核函数有两个作用：①将样本从低维空间→高维空间，使得样本分散开更容易分开；②可实现低维运算（计算量小），分类效果在高维空间表现。<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRMg2D.png" width = "50%" height = "25%" alt="SMOTE.png"><ul><li><div align=left>对偶形式<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRQ3sH.png" width = "30%" height = 15%" alt="SMOTE.png"></li><li><div align=left>决策函数<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRQ8Ld.png" width = "30%" height = "20%" alt="SMOTE.png"></li></ul><div align=left><p>图形形式为<div align=center><img src="https://s2.ax1x.com/2019/12/14/QRM2xe.png" width = "35%" height = "30%" alt="SMOTE.png"></p><div align=left><h2 id="1-4-多分类问题解决思路"><a href="#1-4-多分类问题解决思路" class="headerlink" title="1.4 多分类问题解决思路"></a><div align=left>1.4 多分类问题解决思路</h2><ul><li><p><strong>one-against-all</strong>：如有4个类别的样本1、2、3、4，第一次分类将第1类样本作为正类，其他均为负类，从而进行一次二分类。同理，一共需要进行4次二分类问题，得出的结果分别为f1(x)、f2(x)、f3(x)、f4(x)，分类时将未知样本分类为具有最大分类函数值的那类。</p></li><li><p><strong>one-against-one</strong>：在任意两类样本之间设计一个SVM，因此k个类别的样本就需要设计k(k-1)/2个SVM。当对一个未知样本进行分类时，最后得票最多的类别即为该未知样本的类别。</p></li></ul><hr><h1 id="3-SVM解决回归问题"><a href="#3-SVM解决回归问题" class="headerlink" title="3. SVM解决回归问题"></a><div align=left>3. SVM解决回归问题</h1><div align=center><img src="https://s2.ax1x.com/2019/12/14/QRJIOO.png" width = "35%" height = "25%" alt="SMOTE.png"><ul><li><div align=left>原理概述：在SVM分类的基础上引入了不敏感损失函数，从而得到了回归型支持向量机（Support Vector Machine for Regression，SVR）。</li></ul><div align=center><img src="https://s2.ax1x.com/2019/12/14/QRJ70e.png" width = "30%" height = "13%" alt="SMOTE.png"><ul><li><div align=left>基本思想：SVM应用于回归拟合分析时，其基本思想不再是寻找一个最优分类面使得两类样本分开，而是寻找一个最优分类面使得所有训练样本离该最优分类面的**误差最小**</li><li><div align=left>原始形式</li></ul><div align=center><img src="https://s2.ax1x.com/2019/12/14/QRJ56K.png" width = "30%" height = "13%" alt="SMOTE.png"><ul><li><div align=left>对偶形式</li></ul><div align=center><img src="https://s2.ax1x.com/2019/12/14/QRJHTH.png" width = "45%" height = "16%" alt="SMOTE.png"><ul><li><div align=left>决策函数</li></ul><div align=center><img src="https://s2.ax1x.com/2019/12/14/QRJqkd.png" width = "40%" height = "30%" alt="SMOTE.png"><div align=left><h1 id="3-SVM训练算法"><a href="#3-SVM训练算法" class="headerlink" title="3. SVM训练算法"></a><div align=left>3. SVM训练算法</h1><h2 id="3-1-块算法（chunking-algorithm）"><a href="#3-1-块算法（chunking-algorithm）" class="headerlink" title="3.1 块算法（chunking algorithm）"></a>3.1 块算法（chunking algorithm）</h2><ul><li>基本原理：删除矩阵中对应Lagrange乘数为零的行和列将不会影响最终的结果。对于给定的样本， Chunking算法的目标是通过某种迭代方式逐步排除非支持向量，从而降低训练过程对存储器容量的要求</li><li>具体做法：将一个大型QP问题分解为一系列较小规模的QP问题， 然后找到所有非零的Lagrange乘数并删除。在算法的每步中Chunking都解决一个QP问题，<strong>其样本为上一步所剩的具有非零Lagrange乘数的样本以及M个不满足KKT条件的最差样本</strong>。如果在某一步中，不满足KKT条件的样本数不足M个，则这些样本全部加入到新的QP问题中。<strong>每个QP子问题都采用上一个QP子问题的结果作为初始值</strong>。在算法进行到最后一步时，所有非零Lagrange乘数都被找到，从而解决了初始的大型QP问题。</li><li>优点：Chunking算法将矩阵规模从训练样本数的平方减少到具有非零Lagrange乘数的样本数的平方，在很大程度上降低了训练过程对存储容量的要求。</li><li>缺点：Chunking算法能够大大提高训练速度，尤其是当支持向量的数目远远小于训练样本的数目时。然而，如果支持向量个数比较多，随着算法迭代次数的增多，所选的块也会越来越大，算法的训练速度依旧会变得十分缓慢。</li></ul><h2 id="3-2-分解算法-decomposition-algorithm"><a href="#3-2-分解算法-decomposition-algorithm" class="headerlink" title="3.2 分解算法(decomposition algorithm)"></a>3.2 分解算法(decomposition algorithm)</h2><ul><li>分解算法是目前有效解决大规模问题的主要方法。</li><li>具体做法：分解算法将二次规划问题分解成一系列规模较小的二次规划子问题，进行迭代求解。在每次迭代中，选取拉格朗日乘子分量的一个子集做为工作集，利用传统优化算法求解一个二次规划的子问题。关键在于选择一种<strong>最优工作集选择算法</strong>，而在工作集的选取中采用了随机的方法，因此限制了算法的收敛速度。</li></ul><blockquote><p>【例子】以分类SVM为例，分解算法的主要思想是将训练样本分成工作集B和非工作集N，工作集B中的样本个数为 q ， q 远小于训练样本总数。每次只针对工作集B中的样本进行训练，而固定N中的训练样本。</p></blockquote><ul><li>改进算法：<ul><li>文献 <strong><em>JOACHIMS T. Making large-scale support vector machine learning practical[C]//Advances in Kernel Methods:Support Vector Learning. Cambridge, MA: The MIT Press,1998.</em></strong> 在分解算法的基础上对工作集的选择做了重要改进。采用类似可行方向法的策略确定工作集B。如果存在不满足KTT条件的样本，利用最速下降法，在最速下降方向中存在 q 个样本，然后以这 q 个样本构成工作集， 在该工作集上解决QP问题，直到所有样本满足KTT条件。如此改进提高了分解算法的收敛速度，并且实现了SVMlight算法。</li><li><strong>代表算法SMO（Sequential Minimal Optimization，SMO）</strong><ul><li>介绍：它是分解算法的一个特例，工作集中只有2个样本，其优点是针对2个样本的二次规划问题可以有解析解的形式，从而避免多样本情况下的数值解不稳定及耗时问题，且不需要大的矩阵存储空间，特别适合稀疏样本。工作集的选择不是传统的最陡下降法，而是启发式。通过两个嵌套的循环寻找待优化的样本，然后在内环中选择另一个样本，完成一次优化，再循环，进行下一次优化，直到全部样本都满足最优条件。 SMO算法主要耗时在最优条件的判断上，所以应寻找最合理即计算代价最低的最优条件判别式。</li><li>改进：SMO算法提出后，许多学者对其进行了有效的改进。文献 <strong><em>DAI Liu-ling, HUANG He-yan, CHEN Zhao-xiong.Ternary sequential analytic optimization algorithm for SVM classifier design[J]. Asian Journal of Information Technology, 2005, 4(3): 2-8</em></strong>提出了在内循环中每次优化3个变量，因为3个变量的优化问题同样可以解析求解， 实验表明该算法比SMO的训练时间更短。文献 <strong><em>KEERTHI S S, SHEVADE S K, BHATTAEHARYYA C. Improvements to platt’s SMO algorithm for SVM classifier design[J]. Neural Computation, 2001, 13(3): 637- 649. 和 PLATT J. Using analytic QP and sparseness to speed training support vector machines[C] Advances in Neural Information Processing Systems. [S.l]: MIT Press, 1999,557-563.</em></strong>在迭代过程中的判优条件和循环策略上做了一定的修改，加快了算法的速度。</li></ul></li></ul></li></ul><h2 id="3-3-增量算法-incremental-algorithm"><a href="#3-3-增量算法-incremental-algorithm" class="headerlink" title="3.3 增量算法(incremental algorithm)"></a>3.3 增量算法(incremental algorithm)</h2><ul><li>概念：机器学习系统在处理新增样本时，能够只对原学习结果中与新样本有关的部分进行增加修改或删除操作，与之无关的部分则不被触及。增量训练算法的一个<strong>突出特点</strong>是支持向量机的学习不是一次离线进行的，而是一个数据逐一加入反复优化的过程。</li><li>发展完善<ul><li>文献 <strong><em>SYED N, LIU H, SUNG K. Incremental learning with support vector machines[C] International Joint Conference on Artificial Intelligence. Sweden: Morgan kaufmann publishers, 1999: 352-356.</em></strong> 最早提出了SVM增量训练算法，每次只选一小批常规二次算法能处理的数据作为增量，保留原样本中的支持向量和新增样本混合训练，直到训练样本用完。</li><li>文献 <strong><em>GAUWENBERGHS G, POGGIO T. Incremental and decremental support vector machine[J]. Machine Learning.2001, 44 (13): 409- 415.</em></strong> 提出了增量训练的精确解 ， 即 增 加 一 个 训 练 样 本 或 减 少 一 个 样 本 对Lagrange系数和支持向量的影响。</li><li>文献 <strong><em>RALAIVOLA L, FLORENCE D’ALCHÉ-BUC.Incremental support vector machine learning: a local approach[C] Proceedings of International Conference on Neural Networks. Vienna, Austria: [s.n.], 2001: 322-330.</em></strong> 提出了另一种增量式学习方法，其思想是基于高斯核的局部特性，只更新对学习机器输出影响最大的Lagrange系数，以减少计算复杂度。</li><li>文献 <strong><em>KONG Rui, ZHANG Bing. A fast incremental learning algorithm for support vector machine[J]. Control And Decision, 2005, 20(1): 1129- 1132.</em></strong> 提出了一种“快速增量学习算法”， 该算法依据边界向量不一定是支持向量，但支持向量一定是边界向量的原理，首先选择那些可能成为支持向量的边界向量，进行SVM的增量学习，找出支持向量，最终求出最优分类面，提高训练速度。</li><li>文献 <strong><em>孔波, 刘小茂, 张钧. 基于中心距离比值的增量支持向量机[J]. 计算机应用, 2006, 26(6): 1434-1436.KONG Bo, LIU Xiao-mao, ZHANG Jun. Incremental support vector machine based on center distance ratio[J].Journal of Computer Applications, 2006, 26(6): 1434-1436.</em></strong> 提出了基于中心距离比值的增量运动向量机，利用中心距离比值，在保证训练和测试准确率没有改变的情况下，提高收敛速度。</li><li>文献 <strong><em>李东晖, 杜树新, 吴铁军. 基于壳向量的线性支持向量机快速增量学习算法[J]. 浙江大学学报, 2006, 40(2):202- 215.</em></strong> 提出了基于壳向量的线性SVM增量学习算法，通过初始样本集求得壳向量，进行SVM训练，得到支持向量集降低二次规划过程的复杂度，提高整个算法的训练速度和分类精度。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;🎵听说音乐和学习更配哟🎵&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;【注：本文侧重应用，不讲推导】&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&quot;1-SVM概述&quot;&gt;&lt;a href=&quot;#1-SVM概述&quot; class=&quot;headerlink&quot; title=&quot;1. SVM概述&quot;&gt;&lt;/a&gt;1. SVM概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;类型：有监督学习&lt;/li&gt;
&lt;li&gt;目的：在高维或无限维空间中构造一个超平面或一组超平面，可用于分类、回归或其他任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;若样本为 n 维，则超平面为 n-1 维&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="https://liamwang666.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://liamwang666.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记1《Classification-on-Imbalanced-Data-Sets-Taking-Advantage-of-Errors-to-Improve-Performance》</title>
    <link href="https://liamwang666.github.io/2019/11/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B01%E3%80%8AClassification-on-Imbalanced-Data-Sets-Taking-Advantage-of-Errors-to-Improve-Performance%E3%80%8B-1/"/>
    <id>https://liamwang666.github.io/2019/11/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B01%E3%80%8AClassification-on-Imbalanced-Data-Sets-Taking-Advantage-of-Errors-to-Improve-Performance%E3%80%8B-1/</id>
    <published>2019-11-30T14:15:08.000Z</published>
    <updated>2019-12-16T06:46:50.524Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="1-提要（背景、方法、结果、改进思路）"><a href="#1-提要（背景、方法、结果、改进思路）" class="headerlink" title="1. 提要（背景、方法、结果、改进思路）"></a>1. 提要（背景、方法、结果、改进思路）</h1><h3 id="（1）背景"><a href="#（1）背景" class="headerlink" title="（1）背景"></a>（1）背景</h3><p>当前已提出的解决非平衡样本问题的方法中，大部分都是合成实例来平衡数据集，但往往会导致过拟合问题。</p><blockquote><p><strong><em>数据不平衡领域</em></strong>：软件缺陷检测、医疗诊断、电信欺诈检验、财务风险、DNA测序等</p></blockquote><a id="more"></a>  <h3 id="（2）方法"><a href="#（2）方法" class="headerlink" title="（2）方法"></a>（2）方法</h3><p>本文方法分两个阶段进行。</p><ul><li>第一阶段：检测出对于分类方法难以正确预测的实例，然后将这些实例分为“嘈杂”和“安全”，其中前者是指其最近邻居中的大多数属于相反类别的那些实例。</li><li>第二阶段：根据不平衡率，为难以正确预测的每个实例自适应地合成不同数量的实例。</li></ul><blockquote><p><strong><em>特别之处</em></strong>：利用预测正确性的观察结果识别输入空间的困难区域（无论分类方法或类分布如何）</p></blockquote><h3 id="（3）结果"><a href="#（3）结果" class="headerlink" title="（3）结果"></a>（3）结果</h3><p>分类器的AUC区域得到了显著改善</p><blockquote><p><strong><em>AUC ROC测量方法的意义</em></strong>：可以抓住非平衡样本分类中的矛盾需求，即在以最小误差检验小众类实例的同时，保证分类器对大众类实例的准确性不被严重破坏。</p></blockquote><h3 id="（4）待解决问题"><a href="#（4）待解决问题" class="headerlink" title="（4）待解决问题"></a>（4）待解决问题</h3><h3 id="（5）改进思路"><a href="#（5）改进思路" class="headerlink" title="（5）改进思路"></a>（5）改进思路</h3><hr><h1 id="2-已有方法"><a href="#2-已有方法" class="headerlink" title="2. 已有方法"></a>2. 已有方法</h1><h3 id="（1）外部方法"><a href="#（1）外部方法" class="headerlink" title="（1）外部方法"></a>（1）外部方法</h3><p>在应用分类方法之前对数据集进行预处理使其平衡</p><ul><li>上采样：增加小众类的实例</li><li>下采样：减少大众类的实例</li></ul><blockquote><p><strong><em>代表算法SMOTE</em></strong>：利用小众样本在特征空间的相似性来生成新样本。对于小众样本 xi∈Smin 小众类，从它属于小众类的 K 近邻中随机选取一个样本点  x^i （这个样本是 xi 的；称为临近样本），生成一个新的小众样本 xnew：xnew=xi+(x^i−xi)×δ ，其中 δ∈[0,1]， δ 是随机数。<br>如下图所示：</p><div align=center><img src="https://i.loli.net/2019/12/01/RXZWn4cP5rx9d7k.png" width = "50%" height = "50%" alt="SMOTE.png"></blockquote><blockquote><p><strong><em>SMOTE的变体</em></strong>：在考虑数据的特性（如小众类实例的密度、决策边界或多分类器的融合）的情况下针对输入空间的特殊部分生成小众类实例</p></blockquote><h3 id="（2）内部方法"><a href="#（2）内部方法" class="headerlink" title="（2）内部方法"></a>（2）内部方法</h3><p>调整算法使其更适合处理非平衡样本问题</p><h3 id="（3）融合"><a href="#（3）融合" class="headerlink" title="（3）融合"></a>（3）融合</h3><p>联合多个分类器的结果进预测</p><h3 id="（4）代价敏感法"><a href="#（4）代价敏感法" class="headerlink" title="（4）代价敏感法"></a>（4）代价敏感法</h3><p>使用代价矩阵乘法不同的错误分类</p><h3 id="（5）其他方法"><a href="#（5）其他方法" class="headerlink" title="（5）其他方法"></a>（5）其他方法</h3><p>上述方法的不同结合 &amp; 遗传算法</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-提要（背景、方法、结果、改进思路）&quot;&gt;&lt;a href=&quot;#1-提要（背景、方法、结果、改进思路）&quot; class=&quot;headerlink&quot; title=&quot;1. 提要（背景、方法、结果、改进思路）&quot;&gt;&lt;/a&gt;1. 提要（背景、方法、结果、改进思路）&lt;/h1&gt;&lt;h3 id=&quot;（1）背景&quot;&gt;&lt;a href=&quot;#（1）背景&quot; class=&quot;headerlink&quot; title=&quot;（1）背景&quot;&gt;&lt;/a&gt;（1）背景&lt;/h3&gt;&lt;p&gt;当前已提出的解决非平衡样本问题的方法中，大部分都是合成实例来平衡数据集，但往往会导致过拟合问题。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;数据不平衡领域&lt;/em&gt;&lt;/strong&gt;：软件缺陷检测、医疗诊断、电信欺诈检验、财务风险、DNA测序等&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="论文阅读笔记系列" scheme="https://liamwang666.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97/"/>
    
      <category term="第一篇博客" scheme="https://liamwang666.github.io/tags/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
